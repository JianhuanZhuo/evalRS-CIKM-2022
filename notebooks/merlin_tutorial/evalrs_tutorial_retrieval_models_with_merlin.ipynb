{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial for the EvalRS competition\n",
    "# Retrieval models with Merlin\n",
    "\n",
    "In this notebook, we present a tutorial on how to use the open-source [NVIDIA Merlin](https://github.com/NVIDIA-Merlin/) framework to build and train a retrieval model for EvalRS competition. We are going to use the [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) library for preprocessing and [Merlin Models](https://github.com/NVIDIA-Merlin/models) for building and training Tensorflow-based retrieval models.\n",
    "\n",
    "### Retrieval models\n",
    "Retrieval models are recsys scalable models that are able to retrieve a large number of candidate items for recommendation. They are typically used in two-stage recsys pipelines, where the retrieval stage scores hundreds of thousands or millions of items and then the ranking stage scores the candidate items with more features or with a more powerful architecture.\n",
    "For ML-based candidate retrieval model, as it needs to quickly score millions of items for a given user, the retrieval models typically produce recommendation scores by just computing the dot product between user and item representations. Popular choices of such models are Matrix Factorization, which learns low-rank user and item embeddings, and the Two-Tower architecture, which is a neural network with two MLP towers where both user and item features are fed to generate user and item embeddings in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs EvalRS dependencies\n",
    "#!pip install -r ../../requirements.txt\n",
    "# Installs Merlin dependencies\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Basic imports, read the credentials from the env file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "# Imports EvalRS dependencies\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../upload.env')\n",
    "\n",
    "EMAIL = os.getenv('EMAIL')  # the e-mail you used to sign up\n",
    "assert EMAIL != '' and EMAIL is not None\n",
    "BUCKET_NAME = os.getenv('BUCKET_NAME') # you received it in your e-mail\n",
    "PARTICIPANT_ID = os.getenv('PARTICIPANT_ID') # you received it in your e-mail\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY') # you received it in your e-mail\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY') # you received it in your e-mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Specify some other global variables to improve local iteration and debugging, for example setting a LIMIT to work with a smaller, faster test set_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE: as long as there is a limit specified, the runner won't upload results: make sure to have LIMIT=0 when you want to submit to the leaderboard!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from evaluation.EvalRSRunner import EvalRSRunner\n",
    "from evaluation.EvalRSRunner import ChallengeDataset\n",
    "from reclist.abstractions import RecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LFM dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evalrs_dataset.zip: 100%|██████████████████████████| 677M/677M [01:31<00:00, 7.76MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset.\n",
      "Generating folds.\n",
      "Generating dataset hashes.\n"
     ]
    }
   ],
   "source": [
    "dataset = ChallengeDataset(force_download=False)  # note, if YES, the dataset will be donwloaded again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "runner = EvalRSRunner(\n",
    "    dataset=dataset,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    participant_id=PARTICIPANT_ID,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    email=EMAIL\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates a retrieval recsys pipeline with Merlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "from merlin.dag import ColumnSelector\n",
    "from merlin.schema import Schema, Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 21:59:43.171634: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-08-04 21:59:44.777296: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-04 21:59:45.867956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30480 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-08-04 21:59:45.871984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30973 MB memory:  -> device: 1, name: Quadro GV100, pci bus id: 0000:2d:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import merlin.models.tf as mm\n",
    "from merlin.io import Dataset\n",
    "from merlin.models.tf.dataset import BatchedDataset\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.models.utils import schema_utils\n",
    "from merlin.models.tf.core.transformations import PopularityLogitsCorrection\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_frequencies(train_ds):   \n",
    "    \"\"\"Utility function that returns a TF tensor with the items (tracks) frequency\"\"\"\n",
    "    schema = train_ds.schema\n",
    "    # Gets the item ids cardinality\n",
    "    item_id_feature = schema.select_by_tag(Tags.ITEM_ID)\n",
    "    item_id_feature_name = item_id_feature.column_names[0]\n",
    "    cardinalities = schema_utils.categorical_cardinalities(schema)\n",
    "    item_id_cardinality = cardinalities[item_id_feature_name]\n",
    "\n",
    "    item_id_feature_name = schema.select_by_tag(Tags.ITEM_ID).column_names[0]\n",
    "\n",
    "    item_frequencies_df = (\n",
    "        train_ds.to_ddf()\n",
    "        .groupby(item_id_feature_name)\n",
    "        .size()\n",
    "        .to_frame(\"freq\")\n",
    "        .compute()\n",
    "    )\n",
    "    assert len(item_frequencies_df) <= item_id_cardinality\n",
    "    assert item_frequencies_df.index.max() < item_id_cardinality\n",
    "\n",
    "    # Completing the missing item ids and filling freq with 0\n",
    "    item_frequencies_df = item_frequencies_df.reindex(\n",
    "        np.arange(0, item_id_cardinality)\n",
    "    ).fillna(0)\n",
    "    assert len(item_frequencies_df) == item_id_cardinality\n",
    "\n",
    "    item_frequencies_df = item_frequencies_df.sort_index()\n",
    "    item_frequencies_df[\"dummy\"] = 1\n",
    "    item_frequencies_df[\"expected_id\"] = item_frequencies_df[\"dummy\"].cumsum() - 1\n",
    "    assert (\n",
    "        item_frequencies_df.index == item_frequencies_df[\"expected_id\"]\n",
    "    ).all(), f\"The item id feature ({item_id_feature_name}) should be contiguous from 0 to {item_id_cardinality-1}\"\n",
    "\n",
    "    item_frequencies = tf.convert_to_tensor(\n",
    "        item_frequencies_df[\"freq\"].values\n",
    "    )\n",
    "\n",
    "    return item_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our model for EvalRS by inheriting from the RecModel, which must implement the `train()` and the `predict()` methods. Within `train()` we need to define our full pipeline for preprocessing and training data. \n",
    "\n",
    "#### NVTabular\n",
    "For preprocessing, we use the [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) library, which provides very handful features for common operations like (label) encoding categorical features for embeddings and for normalizing continuous features.\n",
    "NVTabular works with CPUs, uses GPUs and NVIDIA RAPIDS when available for GPU-accelerated preprocessing. For this example we are installing NVTabular using `pip` for CPU usage, but if you want to speedup preprocessing with GPUs you can install NVTabular using `conda` as explained in its [repo](https://github.com/NVIDIA-Merlin/NVTabular).\n",
    "\n",
    "#### Merlin Models\n",
    "For model definition and training we use [Merlin Models](https://github.com/NVIDIA-Merlin/models). It provides a Tensorflow (Keras) API where you can easily build state-of-the-art retrieval and ranking models. One of the core ideas of Models is to leverage the schema generated during preprocessing by **NVTabular** to create automatically the necessary embedding tables for categorical features and define the target of the model.\n",
    "Here we will be implementing two retrieval models: `MatrixFactorizationModel` and `TwoTowerModel`. You can find more information about retrieval with Merlin Models in this [example notebook](https://github.com/NVIDIA-Merlin/models/blob/main/examples/05-Retrieval-Model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRetrievalModel(RecModel):\n",
    "    \n",
    "    def __init__(self, items_df: pd.DataFrame, users_df: pd.DataFrame, top_k: int = 100, \n",
    "                 predict_batch_size=1024, **kwargs):\n",
    "        super(RecModel, self).__init__()\n",
    "        self.items_df = items_df\n",
    "        self.users_df = users_df\n",
    "        self.top_k = top_k\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "        self.hparams = kwargs\n",
    "    \n",
    "    def get_nvtabular_preproc_workflow(self):\n",
    "        \"\"\"Defines an NVTabular preprocessing workflow\"\"\"\n",
    "        \n",
    "        user_id_col = [\"user_id\"] \n",
    "        user_cat_cols = [\"country\", \"gender\"]\n",
    "        user_age_col = [\"age\"]\n",
    "\n",
    "        user_continuous_cols = [\n",
    "            'novelty_artist_avg_month', 'novelty_artist_avg_6months', 'novelty_artist_avg_year',\n",
    "            'mainstreaminess_avg_month', 'mainstreaminess_avg_6months', 'mainstreaminess_avg_year', 'mainstreaminess_global',\n",
    "            'relative_le_per_weekday1', 'relative_le_per_weekday2', 'relative_le_per_weekday3',\n",
    "             'relative_le_per_weekday4', 'relative_le_per_weekday5','relative_le_per_weekday6', 'relative_le_per_weekday7',\n",
    "             'relative_le_per_hour0', 'relative_le_per_hour1','relative_le_per_hour2', 'relative_le_per_hour3',\n",
    "             'relative_le_per_hour4', 'relative_le_per_hour5', 'relative_le_per_hour6', 'relative_le_per_hour7', \n",
    "             'relative_le_per_hour8', 'relative_le_per_hour9', 'relative_le_per_hour10', 'relative_le_per_hour11',\n",
    "             'relative_le_per_hour12', 'relative_le_per_hour13', 'relative_le_per_hour14', 'relative_le_per_hour15',\n",
    "             'relative_le_per_hour16', 'relative_le_per_hour17','relative_le_per_hour18', 'relative_le_per_hour19',\n",
    "             'relative_le_per_hour20', 'relative_le_per_hour21', 'relative_le_per_hour22', 'relative_le_per_hour23', \n",
    "        ]\n",
    "\n",
    "        user_counts_cols = ['playcount', 'cnt_listeningevents', 'cnt_distinct_tracks', \n",
    "                                'cnt_distinct_artists', 'cnt_listeningevents_per_week']\n",
    "\n",
    "        item_id = [\"track_id\"]\n",
    "        item_features_cols = [\"artist_id\", \"album_id\"] \n",
    "\n",
    "\n",
    "        user_id = user_id_col >> ops.Categorify() >> ops.TagAsUserID()\n",
    "        user_feat_cat = user_cat_cols >> ops.Categorify() >> ops.TagAsUserFeatures()\n",
    "        age_boundaries = list(np.arange(0,100,5))\n",
    "        user_age = user_age_col >> ops.FillMissing(0) >> ops.Bucketize(age_boundaries) >> ops.Categorify() >> ops.TagAsUserFeatures()\n",
    "        user_feat_cont = user_continuous_cols >> ops.FillMedian() >> ops.Normalize() >> ops.TagAsUserFeatures()\n",
    "        user_feat_count = user_counts_cols >> ops.Clip(min_value=1) >> ops.FillMedian() >> ops.LogOp() >> ops.Normalize() >> ops.TagAsUserFeatures()\n",
    "        user_features = user_id + user_feat_cat + user_age + user_feat_cont + user_feat_count\n",
    "\n",
    "        item_id = item_id >> ops.Categorify() >> ops.TagAsItemID()\n",
    "        item_cat_feat = item_features_cols >> ops.Categorify() >> ops.TagAsItemFeatures()\n",
    "        item_features = item_id + item_cat_feat\n",
    "\n",
    "        outputs = user_features + item_features\n",
    "        workflow = nvt.Workflow(outputs)\n",
    "        return workflow\n",
    "    \n",
    "    def preprocess_dataset(self, events_df):\n",
    "        \"\"\"Preprocess the dataset using an NVTabular workflow\"\"\"        \n",
    "        nvt_dataset = nvt.Dataset(events_df)\n",
    "        \n",
    "        CATEG_MAPPING_FOLDER = 'categories/'\n",
    "        shutil.rmtree(CATEG_MAPPING_FOLDER, ignore_errors=True)\n",
    "        \n",
    "        nvt_workflow = self.get_nvtabular_preproc_workflow()\n",
    "        nvt_workflow.fit(nvt_dataset)\n",
    "        schema = nvt_workflow.output_schema\n",
    "        \n",
    "        # Loads mapping of categ features after the workflow is fit\n",
    "        self.user_ids_mapping_df = pd.read_parquet(CATEG_MAPPING_FOLDER+'unique.user_id.parquet')[['user_id']]\n",
    "        self.track_ids_mapping_df = pd.read_parquet(CATEG_MAPPING_FOLDER+'unique.track_id.parquet')[['track_id']]\n",
    "        \n",
    "        transformed_dataset = nvt_workflow.transform(nvt_dataset)        \n",
    "        transformed_df = transformed_dataset.persist().repartition(npartitions=10)  \n",
    "        \n",
    "        return transformed_df, schema\n",
    "    \n",
    "        \n",
    "    def get_item_retrieval_task(self):        \n",
    "        \"\"\"Defines the item retrieval task to be used by the retrieval models\"\"\"\n",
    "        items_frequencies = get_item_frequencies(self.train_dataset)\n",
    "        \n",
    "        post_logits = None\n",
    "        reg_factor = self.hparams['logq_correction_factor']\n",
    "        if reg_factor > 0.0:\n",
    "            post_logits = PopularityLogitsCorrection(\n",
    "                items_frequencies, reg_factor=reg_factor, schema=self.schema\n",
    "            )\n",
    "        \n",
    "        item_retrieval_task = mm.ItemRetrievalTask(self.schema, \n",
    "                                        logits_temperature=self.hparams['logits_temperature'],\n",
    "                                         post_logits=post_logits)\n",
    "        \n",
    "        return item_retrieval_task\n",
    "\n",
    "    \n",
    "    def get_model(self):\n",
    "        \"\"\"Defines the model architecture. Needs to be overridden by the child class\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def compile_model(self, model):\n",
    "        \"\"\"Compiles the Keras model setting the metrics, loss, learning rate and optimizer\"\"\"\n",
    "        metrics = [mm.TopKMetricsAggregator(mm.RecallAt(20), mm.MRRAt(20))]\n",
    "        \n",
    "        lerning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            self.hparams['lr'],\n",
    "            decay_steps=self.hparams['lr_decay_steps'],\n",
    "            decay_rate=self.hparams['lr_decay_rate'],\n",
    "            staircase=True,\n",
    "        )\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lerning_rate)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, label_smoothing=self.hparams['label_smoothing'],\n",
    "        )\n",
    "        \n",
    "        model.compile(optimizer, loss=loss, metrics=metrics, run_eagerly=False)\n",
    "        \n",
    "    def get_items_topk_recommender_model(\n",
    "        self,\n",
    "        train_dataset: Dataset, \n",
    "        schema, \n",
    "        model, \n",
    "        ):        \n",
    "        \"\"\"Converts a retrieval model into a Top-k recommender model, which\n",
    "        takes only user features as input, generates the user representations \n",
    "        (e.g. by taking the user embedding for MF or using the user tower to generate it)\n",
    "        and uses scores all cached item representations to return the most similar items \n",
    "        (P.s. This procedure would be done by an ANN engine in production)\"\"\"\n",
    "        item_features = schema.select_by_tag(Tags.ITEM).column_names\n",
    "        item_dataset = train_dataset.to_ddf()[item_features].drop_duplicates(subset=['track_id'], keep='last').compute()\n",
    "        item_dataset = Dataset(item_dataset)\n",
    "\n",
    "        return model.to_top_k_recommender(item_dataset, k=self.top_k)\n",
    "        \n",
    "    def train_model(self, model, train_dataset):\n",
    "        model.fit(\n",
    "            train_dataset,\n",
    "            epochs=self.hparams['epochs'],\n",
    "            batch_size=self.hparams['train_batch_size'],\n",
    "            #steps_per_epoch=5,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            train_metrics_steps=100,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def train(self, train_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Implement here your training logic. Since our example method is a simple random model,\n",
    "        we actually don't use any training data to build the model, but you should ;-)\n",
    "        At the end of training, make sure the class contains a trained model you can use in the predict method.\n",
    "        \"\"\"\n",
    "            \n",
    "        print(\"Merging events and user features\")\n",
    "        events_df = train_df.merge(self.users_df, on='user_id', how='inner')\n",
    "\n",
    "        print(\"Start preprocessing\")\n",
    "        transformed_df, schema = self.preprocess_dataset(events_df)\n",
    "        train_dataset = Dataset(transformed_df, schema=schema)\n",
    "            \n",
    "        self.train_dataset = train_dataset\n",
    "        self.schema = self.train_dataset.schema\n",
    "        \n",
    "        print(\"Building the model\")\n",
    "        model = self.get_model()\n",
    "        self.compile_model(model)                \n",
    "        \n",
    "        print(\"Start training\")\n",
    "        self.train_model(model, train_dataset)        \n",
    "        self.trained_model = model\n",
    "        \n",
    "        print(\"Preparing retrieval model for prediction\")\n",
    "        self.topk_rec_model = self.get_items_topk_recommender_model(train_dataset, schema, model)\n",
    "        \n",
    "        print(\"Caching users transformed features\")\n",
    "        self.users_schema = schema.select_by_tag(Tags.USER)\n",
    "        user_features = self.users_schema.column_names\n",
    "        self.users_transformed_df = train_dataset.to_ddf()[user_features].drop_duplicates(subset=['user_id'], keep='last').compute()\n",
    "        # Adding the raw (original) user id to the dataframe\n",
    "        self.users_transformed_df = self.users_transformed_df.merge(self.user_ids_mapping_df.rename({'user_id': 'raw_user_id'}, axis=1),\n",
    "                                                                    left_on='user_id', right_index=True)\n",
    "\n",
    "        print(\"Training completed!\")         \n",
    "\n",
    "    def predict(self, user_ids: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"        \n",
    "        This function takes as input all the users that we want to predict the top-k items for, and \n",
    "        returns all the predicted songs.\n",
    "\n",
    "        While in this example is just a random generator, the same logic in your implementation \n",
    "        would allow for batch predictions of all the target data points.        \n",
    "        \"\"\"\n",
    "            \n",
    "        self.predict_user_ids = user_ids\n",
    "        \n",
    "        print(\"Start prediction\")\n",
    "        print(\"# users:\",len(user_ids))\n",
    "        test_users_df = user_ids.rename({'user_id': 'raw_user_id'}, axis=1).merge(self.users_transformed_df, \n",
    "                                                       on='raw_user_id', how='left')\n",
    "        self.test_users_df = test_users_df\n",
    "        test_users_found_df = test_users_df[~test_users_df[test_users_df.columns[1]].isna()]\n",
    "        test_users_not_found_df = test_users_df[test_users_df[test_users_df.columns[1]].isna()]        \n",
    "\n",
    "        test_users_dataset = Dataset(test_users_found_df, self.users_schema)\n",
    "        test_batched_dataset = BatchedDataset(\n",
    "            test_users_dataset, batch_size=self.predict_batch_size, shuffle=False, schema=self.users_schema\n",
    "        )        \n",
    "        \n",
    "        print(f\"Predicting Top-{self.top_k} items for test users\")\n",
    "        predictions = self.topk_rec_model.predict(test_batched_dataset)[1].astype(np.int32) \n",
    "        \n",
    "        print(f\"Converting user ids and predicted item ids to the original ids\")\n",
    "        predictions_converted = self.convert_prediction_item_ids(predictions)\n",
    "        \n",
    "        user_ids_found_converted = test_users_found_df['raw_user_id'].values.astype(np.int32)        \n",
    "        user_ids_not_found_converted = test_users_not_found_df['raw_user_id'].values.astype(np.int32)\n",
    "        \n",
    "        # Merging raw user id with top-k predictions\n",
    "        user_predictions_found_converted = np.concatenate((np.expand_dims(user_ids_found_converted, -1), predictions_converted), axis=1)\n",
    "        user_predictions_not_found_converted = np.concatenate((np.expand_dims(user_ids_not_found_converted, -1), \n",
    "                                                               np.zeros((user_ids_not_found_converted.shape[0], predictions_converted.shape[1]))), axis=1)\n",
    "        # Combining predictions of users found and not found\n",
    "        user_predictions_converted = np.vstack([user_predictions_found_converted, user_predictions_not_found_converted])        \n",
    "        user_predictions_df = pd.DataFrame(user_predictions_converted, columns=['user_id', *[str(i) for i in range(predictions_converted.shape[1])]]).set_index('user_id')\n",
    "        # Ensures predictions output dataframe is sorted the same as input user_ids order\n",
    "        user_predictions_df = user_predictions_df.loc[user_ids['user_id'].values]\n",
    "        self.user_predictions_df = user_predictions_df\n",
    "        print(\"Finish prediction\")\n",
    "        return user_predictions_df\n",
    "    \n",
    "    def convert_user_ids(self, user_ids):\n",
    "        \"\"\"Converts the encoded user ids into the original ids\"\"\"\n",
    "        raw_user_ids = self.user_ids_mapping_df['user_id'].loc[user_ids].values\n",
    "        return raw_user_ids\n",
    "    \n",
    "    def convert_prediction_item_ids(self,predictions):\n",
    "        \"\"\"Converts the encoded predicted item ids into the original item ids\"\"\"\n",
    "        raw_topk_predicted_item_ids = self.track_ids_mapping_df['track_id'].loc[predictions.reshape(-1)].values\n",
    "        raw_topk_predicted_item_ids = np.reshape(raw_topk_predicted_item_ids, (-1, predictions.shape[1]))\n",
    "        return raw_topk_predicted_item_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMFModel(MyRetrievalModel):\n",
    "    \n",
    "    def get_model(self):      \n",
    "        item_retrieval_task = self.get_item_retrieval_task()\n",
    "\n",
    "        model = mm.MatrixFactorizationModel(\n",
    "            self.schema,\n",
    "            dim=self.hparams['mf_dim'],\n",
    "            prediction_tasks=item_retrieval_task,\n",
    "            embeddings_l2_reg=self.hparams['embeddings_l2_reg']\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_model = MyMFModel(\n",
    "    items_df=dataset.df_tracks,\n",
    "    users_df=dataset.df_users,\n",
    "    \n",
    "    # Training hparams\n",
    "    epochs=5,\n",
    "    train_batch_size=8192,\n",
    "    lr=1e-3,\n",
    "    lr_decay_steps=100,\n",
    "    lr_decay_rate=0.96,\n",
    "    label_smoothing=0.0,\n",
    "    \n",
    "    # Model hparams\n",
    "    logq_correction_factor=1.0,\n",
    "    embeddings_l2_reg=5e-6,\n",
    "    logits_temperature=1.8,\n",
    "    mf_dim=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finally, we run the evaluation code: remember, if LIMIT is not 0, your submission won't be uploaded but the loop may still be useful for you to debug / iterate locally_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin Evaluation... \n",
      "\n",
      "Performing Training for fold 1/4...\n",
      "Merging events and user features\n",
      "Start preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n",
      "Start training\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f91f48ff520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f91f48ff520>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837/837 [==============================] - 52s 54ms/step - loss: 9.1677 - recall_at_20: 0.0025 - mrr_at_20: 4.8577e-04 - regularization_loss: 0.0164\n",
      "Epoch 2/5\n",
      "837/837 [==============================] - 43s 51ms/step - loss: 9.0334 - recall_at_20: 0.0074 - mrr_at_20: 0.0014 - regularization_loss: 0.0641\n",
      "Epoch 3/5\n",
      "837/837 [==============================] - 45s 53ms/step - loss: 8.8135 - recall_at_20: 0.0188 - mrr_at_20: 0.0041 - regularization_loss: 0.1610\n",
      "Epoch 4/5\n",
      "837/837 [==============================] - 49s 58ms/step - loss: 8.6852 - recall_at_20: 0.0291 - mrr_at_20: 0.0059 - regularization_loss: 0.2296\n",
      "Epoch 5/5\n",
      "837/837 [==============================] - 49s 58ms/step - loss: 8.6165 - recall_at_20: 0.0372 - mrr_at_20: 0.0082 - regularization_loss: 0.2701\n",
      "Preparing retrieval model for prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "WARNING:absl:Found untraced functions such as model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, sequential_block_6_layer_call_fn, sequential_block_6_layer_call_and_return_conditional_losses, concat_features_1_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpey_v0yy3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpey_v0yy3/assets\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching users transformed features\n",
      "Training completed!\n",
      "Performing Evaluation for fold 1/4...\n",
      "Start prediction\n",
      "# users: 29725\n",
      "Predicting Top-100 items for test users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 3s 78ms/step\n",
      "Converting user ids and predicted item ids to the original ids\n",
      "Finish prediction\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : stats\n",
      "Test Description : \n",
      "Test Result      : {'num_users': 29725, 'max_items': 1, 'min_items': 1}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : HIT_RATE\n",
      "Test Description : \n",
      "Test Result      : 0.036198486122792264\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : MRR\n",
      "Test Description : \n",
      "Test Result      : 0.002551413919850455\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_COUNTRY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.010613831227404788, 'fpr': 0.962824427480916, 'BR': 0.9713679745493107, 'CA': 0.9553903345724907, 'DE': 0.9758713136729222, 'ES': 0.9642857142857143, 'FI': 0.9882352941176471, 'FR': 0.9477611940298507, 'NL': 0.9633802816901409, 'NaN': 0.956132720268423, 'PL': 0.9781368821292775, 'RU': 0.9790491539081386, 'SE': 0.9705882352941176, 'UA': 0.9710144927536232, 'UK': 0.9746954076850984, 'US': 0.9738485558157689}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_USER_ACTIVITY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.005170959007157638, 'fpr': 0.9638015138772077, 10: 0.9619973486522315, 100: 0.96406672098109, 1000: 0.9635340186000979, 10000: 0.9454545454545454}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_ARTIST_POPULARITY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.040376216203376544, 'fpr': 0.9638015138772077, 10: 0.9977744807121661, 100: 0.9914035087719298, 1000: 0.9770555990602976, 10000: 0.9267645858833129, 100000: 0.8737864077669902}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_GENDER\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.008118437614388388, 'fpr': 0.9638015138772077, 'f': 0.9740865509199275, 'm': 0.970973307622044, 'n': 0.9569030318215986}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : BEING_LESS_WRONG\n",
      "Test Description : \n",
      "Test Result      : 0.3181964159011841\n",
      "\n",
      "Generating reports at 2022-08-05 01:05:43.577245\n",
      "\n",
      "Performing Training for fold 2/4...\n",
      "Merging events and user features\n",
      "Start preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n",
      "Start training\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:merlin_models:The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840/840 [==============================] - 50s 57ms/step - loss: 9.1662 - recall_at_20: 0.0027 - mrr_at_20: 4.6675e-04 - regularization_loss: 0.0164\n",
      "Epoch 2/5\n",
      "840/840 [==============================] - 49s 58ms/step - loss: 9.0297 - recall_at_20: 0.0090 - mrr_at_20: 0.0015 - regularization_loss: 0.0645\n",
      "Epoch 3/5\n",
      "840/840 [==============================] - 49s 57ms/step - loss: 8.8097 - recall_at_20: 0.0194 - mrr_at_20: 0.0042 - regularization_loss: 0.1625\n",
      "Epoch 4/5\n",
      "840/840 [==============================] - 49s 57ms/step - loss: 8.6857 - recall_at_20: 0.0295 - mrr_at_20: 0.0064 - regularization_loss: 0.2296\n",
      "Epoch 5/5\n",
      "840/840 [==============================] - 49s 57ms/step - loss: 8.6187 - recall_at_20: 0.0373 - mrr_at_20: 0.0085 - regularization_loss: 0.2689\n",
      "Preparing retrieval model for prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "WARNING:absl:Found untraced functions such as model_context_1_layer_call_fn, model_context_1_layer_call_and_return_conditional_losses, sequential_block_14_layer_call_fn, sequential_block_14_layer_call_and_return_conditional_losses, concat_features_3_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmplgvjq8je/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmplgvjq8je/assets\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching users transformed features\n",
      "Training completed!\n",
      "Performing Evaluation for fold 2/4...\n",
      "Start prediction\n",
      "# users: 29724\n",
      "Predicting Top-100 items for test users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 78ms/step\n",
      "Converting user ids and predicted item ids to the original ids\n",
      "Finish prediction\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : stats\n",
      "Test Description : \n",
      "Test Result      : {'num_users': 29724, 'max_items': 1, 'min_items': 1}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : HIT_RATE\n",
      "Test Description : \n",
      "Test Result      : 0.03445027587134975\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : MRR\n",
      "Test Description : \n",
      "Test Result      : 0.0026599950335531226\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_COUNTRY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.010449930226727158, 'fpr': 0.9645311545510079, 'BR': 0.968520461699895, 'CA': 0.9714285714285714, 'DE': 0.9800543970988214, 'ES': 0.9807692307692307, 'FI': 0.9716713881019831, 'FR': 0.9838709677419355, 'NL': 0.962536023054755, 'NaN': 0.9588003255901321, 'PL': 0.9748369058713886, 'RU': 0.9764150943396226, 'SE': 0.9877300613496932, 'UA': 0.9773584905660377, 'UK': 0.970722781335773, 'US': 0.9695685670261941}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_USER_ACTIVITY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.016223116750988725, 'fpr': 0.9655497241286503, 10: 0.9632486388384754, 100: 0.9674723579326305, 1000: 0.9566375968992248, 10000: 0.9137931034482759}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_ARTIST_POPULARITY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.05086752777712962, 'fpr': 0.9655497241286503, 10: 0.9984076433121019, 100: 0.9887482419127989, 1000: 0.9811665607120152, 10000: 0.9302532647407994, 100000: 0.8181818181818182}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_GENDER\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.006533419105405169, 'fpr': 0.9655497241286503, 'f': 0.9699481865284975, 'm': 0.9741457059825633, 'n': 0.958943911066195}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : BEING_LESS_WRONG\n",
      "Test Description : \n",
      "Test Result      : 0.31445106863975525\n",
      "\n",
      "Generating reports at 2022-08-05 01:11:44.232392\n",
      "\n",
      "Performing Training for fold 3/4...\n",
      "Merging events and user features\n",
      "Start preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n",
      "Start training\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:merlin_models:The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839/839 [==============================] - 46s 51ms/step - loss: 9.1679 - recall_at_20: 0.0024 - mrr_at_20: 5.2434e-04 - regularization_loss: 0.0164\n",
      "Epoch 2/5\n",
      "839/839 [==============================] - 44s 52ms/step - loss: 9.0326 - recall_at_20: 0.0087 - mrr_at_20: 0.0014 - regularization_loss: 0.0647\n",
      "Epoch 3/5\n",
      "839/839 [==============================] - 44s 52ms/step - loss: 8.8137 - recall_at_20: 0.0189 - mrr_at_20: 0.0039 - regularization_loss: 0.1619\n",
      "Epoch 4/5\n",
      "839/839 [==============================] - 49s 58ms/step - loss: 8.6865 - recall_at_20: 0.0301 - mrr_at_20: 0.0064 - regularization_loss: 0.2299\n",
      "Epoch 5/5\n",
      "839/839 [==============================] - 48s 57ms/step - loss: 8.6180 - recall_at_20: 0.0378 - mrr_at_20: 0.0083 - regularization_loss: 0.2700\n",
      "Preparing retrieval model for prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "WARNING:absl:Found untraced functions such as model_context_2_layer_call_fn, model_context_2_layer_call_and_return_conditional_losses, sequential_block_22_layer_call_fn, sequential_block_22_layer_call_and_return_conditional_losses, concat_features_5_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpf_mmnjk4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpf_mmnjk4/assets\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching users transformed features\n",
      "Training completed!\n",
      "Performing Evaluation for fold 3/4...\n",
      "Start prediction\n",
      "# users: 29710\n",
      "Predicting Top-100 items for test users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 3s 79ms/step\n",
      "Converting user ids and predicted item ids to the original ids\n",
      "Finish prediction\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : stats\n",
      "Test Description : \n",
      "Test Result      : {'num_users': 29710, 'max_items': 1, 'min_items': 1}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : HIT_RATE\n",
      "Test Description : \n",
      "Test Result      : 0.03136990912150791\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : MRR\n",
      "Test Description : \n",
      "Test Result      : 0.0025131002584871183\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_COUNTRY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.00846380603040132, 'fpr': 0.9676300137257893, 'BR': 0.9768339768339769, 'CA': 0.9685314685314685, 'DE': 0.9756915339480302, 'ES': 0.9507042253521126, 'FI': 0.9751552795031055, 'FR': 0.983739837398374, 'NL': 0.9818731117824774, 'NaN': 0.9641176839596011, 'PL': 0.9728506787330317, 'RU': 0.9763539282990084, 'SE': 0.980327868852459, 'UA': 0.975177304964539, 'UK': 0.964221824686941, 'US': 0.9720421393841167}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_USER_ACTIVITY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.0016313683147440028, 'fpr': 0.9686300908784921, 10: 0.9662408759124088, 100: 0.9692896639727775, 1000: 0.9660415610745058, 10000: 0.967741935483871}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_ARTIST_POPULARITY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.028774270493335585, 'fpr': 0.9686300908784921, 10: 0.9992383853769993, 100: 0.9915119363395225, 1000: 0.981967731730465, 10000: 0.9352166101018168, 100000: 0.925}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_GENDER\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.004534943196827568, 'fpr': 0.9686300908784921, 'f': 0.9717368961973278, 'm': 0.9746025397460254, 'n': 0.9641045154743785}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : BEING_LESS_WRONG\n",
      "Test Description : \n",
      "Test Result      : 0.3127131164073944\n",
      "\n",
      "Generating reports at 2022-08-05 01:17:36.839814\n",
      "\n",
      "Performing Training for fold 4/4...\n",
      "Merging events and user features\n",
      "Start preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n",
      "Start training\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:merlin_models:The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839/839 [==============================] - 50s 57ms/step - loss: 9.1672 - recall_at_20: 0.0026 - mrr_at_20: 5.2333e-04 - regularization_loss: 0.0164\n",
      "Epoch 2/5\n",
      "839/839 [==============================] - 49s 57ms/step - loss: 9.0304 - recall_at_20: 0.0076 - mrr_at_20: 0.0015 - regularization_loss: 0.0649\n",
      "Epoch 3/5\n",
      "839/839 [==============================] - 48s 57ms/step - loss: 8.8102 - recall_at_20: 0.0191 - mrr_at_20: 0.0042 - regularization_loss: 0.1632\n",
      "Epoch 4/5\n",
      "839/839 [==============================] - 48s 57ms/step - loss: 8.6832 - recall_at_20: 0.0304 - mrr_at_20: 0.0064 - regularization_loss: 0.2312\n",
      "Epoch 5/5\n",
      "839/839 [==============================] - 47s 55ms/step - loss: 8.6142 - recall_at_20: 0.0379 - mrr_at_20: 0.0086 - regularization_loss: 0.2714\n",
      "Preparing retrieval model for prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "WARNING:absl:Found untraced functions such as model_context_3_layer_call_fn, model_context_3_layer_call_and_return_conditional_losses, sequential_block_30_layer_call_fn, sequential_block_30_layer_call_and_return_conditional_losses, concat_features_7_layer_call_fn while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9i6c18ro/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9i6c18ro/assets\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching users transformed features\n",
      "Training completed!\n",
      "Performing Evaluation for fold 4/4...\n",
      "Start prediction\n",
      "# users: 29722\n",
      "Predicting Top-100 items for test users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 66ms/step\n",
      "Converting user ids and predicted item ids to the original ids\n",
      "Finish prediction\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : stats\n",
      "Test Description : \n",
      "Test Result      : {'num_users': 29722, 'max_items': 1, 'min_items': 1}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : HIT_RATE\n",
      "Test Description : \n",
      "Test Result      : 0.033342305363030754\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : MRR\n",
      "Test Description : \n",
      "Test Result      : 0.002640375479373178\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_COUNTRY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.008056225010987683, 'fpr': 0.966379835516296, 'BR': 0.9855222337125129, 'CA': 0.9581749049429658, 'DE': 0.976274165202109, 'ES': 0.9689655172413794, 'FI': 0.978494623655914, 'FR': 0.9739776951672863, 'NL': 0.9737609329446064, 'NaN': 0.9614450327000934, 'PL': 0.9701213818860878, 'RU': 0.972972972972973, 'SE': 0.9685314685314685, 'UA': 0.985239852398524, 'UK': 0.966934763181412, 'US': 0.9754098360655737}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_USER_ACTIVITY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.004802074586842564, 'fpr': 0.9666576946369693, 10: 0.9682683590208522, 100: 0.9673593036651449, 1000: 0.9618739097931722, 10000: 0.9545454545454546}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_ARTIST_POPULARITY\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.04462838558805846, 'fpr': 0.9666576946369693, 10: 1.0, 100: 0.9902482269503546, 1000: 0.9824060498495254, 10000: 0.9289220699529556, 100000: 0.8539325842696629}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : FPED_GENDER\n",
      "Test Description : \n",
      "Test Result      : {'fped': 0.006083124258100832, 'fpr': 0.9666576946369693, 'f': 0.9726762002042901, 'm': 0.9732224363525712, 'n': 0.9609915691455896}\n",
      "\n",
      "============= TEST RESULTS ===============\n",
      "Test Type        : BEING_LESS_WRONG\n",
      "Test Description : \n",
      "Test Result      : 0.31706246733665466\n",
      "\n",
      "Generating reports at 2022-08-05 01:23:33.508874\n",
      "SUBMISSION RESULTS SAVE TO gmoreira_nvidia.com_16596626140211.json\n",
      "Starting submission at 2022-08-05 01:23:34.023098...\n",
      "\n",
      "\n",
      "All done at 2022-08-05 01:23:36.003906: see you, space cowboy!\n",
      "CPU times: user 27min 21s, sys: 4min 22s, total: 31min 44s\n",
      "Wall time: 23min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "runner.evaluate(model=mf_model, limit=LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-tower architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Two-Tower Model consists of item (candidate) and user (query) encoder towers. With two towers, the model can learn representations (embeddings) for queries and candidates separately.\n",
    "\n",
    "<img src=\"./images/TwoTower.png\"  width=\"30%\">\n",
    "\n",
    "Image Adapted from: [Off-policy Learning in Two-stage Recommender Systems](https://dl.acm.org/doi/abs/10.1145/3366423.3380130)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTwoTowerModel(MyRetrievalModel):\n",
    "    \n",
    "    def get_model(self):   \n",
    "        item_retrieval_task = self.get_item_retrieval_task()\n",
    "        \n",
    "        model = mm.TwoTowerModel(\n",
    "            self.schema,\n",
    "            query_tower=mm.MLPBlock(\n",
    "                self.hparams['tt_mlp_layers'],\n",
    "                activation=self.hparams['tt_mlp_activation'],\n",
    "                no_activation_last_layer=True,    \n",
    "                dropout=self.hparams['tt_mlp_dropout'],                \n",
    "                kernel_regularizer=regularizers.l2(self.hparams['tt_mlp_l2_reg']),\n",
    "                bias_regularizer=regularizers.l2(self.hparams['tt_mlp_l2_reg']),\n",
    "            ),\n",
    "            embedding_options=mm.EmbeddingOptions(\n",
    "                infer_embedding_sizes=True,\n",
    "                infer_embedding_sizes_multiplier=self.hparams['tt_infer_embedding_sizes_multiplier'],\n",
    "                embeddings_l2_reg=self.hparams['embeddings_l2_reg'],\n",
    "            ),\n",
    "            prediction_tasks=item_retrieval_task\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_model = MyTwoTowerModel(\n",
    "    items_df=dataset.df_tracks,\n",
    "    users_df=dataset.df_users,\n",
    "    \n",
    "    # Training hparams\n",
    "    epochs=5,\n",
    "    train_batch_size=8192,\n",
    "    lr=1e-3,\n",
    "    lr_decay_steps=100,\n",
    "    lr_decay_rate=0.96,\n",
    "    label_smoothing=0.0,\n",
    "    \n",
    "    # Model hparams\n",
    "    logq_correction_factor=1.0,\n",
    "    embeddings_l2_reg=1e-5,\n",
    "    logits_temperature=1.8,\n",
    "    tt_mlp_layers=[128,64],\n",
    "    tt_mlp_activation=\"relu\",\n",
    "    tt_mlp_dropout=0.3,\n",
    "    tt_mlp_l2_reg=5e-5,\n",
    "    tt_infer_embedding_sizes_multiplier=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finally, we run the evaluation code: remember, if LIMIT is not 0, your submission won't be uploaded but the loop may still be useful for you to debug / iterate locally_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin Evaluation... \n",
      "\n",
      "Performing Training for fold 1/4...\n",
      "Merging events and user features\n",
      "Start preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/home/gmoreira/projects/nvidia/nvidia_merlin/core/merlin/io/dataset.py:251: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "runner.evaluate(model=tt_model, limit=LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you have learned how build retrieval models (MF, Two-Tower) with the [Merlin](https://github.com/NVIDIA-Merlin/) open-source framework for the EvalRS competition. \n",
    "Feel free to improve these models using Tensorflow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
